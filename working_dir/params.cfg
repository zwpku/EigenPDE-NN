[default]
dim = 1
pot_id = 2
beta = 1.5
output_dir = ./data/
eig_file_name_prefix = eigen_vector
data_filename_prefix = states_800ns_1e0
log_filename = log.txt
eig_idx_k = 2
#compute_all_k_eigs = False
compute_all_k_eigs = True
namd_data_flag = True
#namd_data_flag = False
#weights in the loss function, separated by ','
eig_weight = 1.0, 0.05, 0.6, 0.3, 0.2, 0.1
#False 
load_data_how_often = 1
[potential]
stiff_eps = 0.50
[sample_data]
delta_t = 0.001
N_state = 1e4
# Coefficient under which the data is sampled
SDE_beta = 1.0
[NAMD]
namd_dcddata_filename_prefix = colvar-together
#test_A
#namd_data_filename_prefix = run
psf_name = vacuum
# Possible value: none, trans, trans-rot
align_data_flag = trans-rot

temperature = 300
# Diffusion coefficient, unit is cm^2/s, default value 10^{-5}
diffusion_coeff = 1e-5
# Possible value: all, nonh, angle_atoms, angle
which_data_to_use = nonh
# If biased MD data are used, the files .colvar.traj 
# and .pmf are also required.
use_biased_data = True

weight_threshold_to_remove_states = 0

# Info for training data
namd_dcddata_path = ../../MD/alanine-dipeptide/abf-fixed-800ns-0.7/
data_filename_prefix = states_800ns_1e0

# Info for validataion data
namd_dcddata_path_validation = ../../MD/alanine-dipeptide/abf-fixed-80ns-0.7/
data_filename_prefix_validation = states_80ns_7e-1
[NeuralNetArch]
# Size of inner layers (without input/output layers)
arch_size_list = 20,20,20
# Activation function. 
# Possible values: ELU, LogSigmoid, ReLU, CELU, Sigmoid, Softplus, Tanh, Tanhshrink
# CELU will be used when function with given name does not exist!
activation_name = Tanh
features = {(dihedral, 12,14,16,0); (dihedral, 2,0,16,14); (bond, 0,1);
    (angle, 4,2,0); (bond, 16,18); (dihedral, 1,0,2,4); (angle, 8, 12, 13)}
[Training]
# total training step
train_max_step = 20000
# Whether to start from a trained model 
load_init_model = False
# Filename of trained model
init_model_name = eigen_vector.pt
# the following lists should have the same length
# step at which a new stage starts
stage_list = 0, 4000, 8000, 12000, 16000 
# parameters for each stage: batch-size, learning rate, and two penalty constants
batch_size_list = 10000, 10000, 10000, 10000, 10000
batch_uniform_weight = True
learning_rate_list = 1e-3, 1e-3, 1e-3, 1e-3, 1e-3
alpha_1_list = 20.0, 20.0, 20.0, 20.0, 20.0
alpha_2_list = 20.0, 20.0, 20.0, 20.0, 20.0

write_feature_to_file = False

# Use Rayleigh quotient or energy in loss function
use_Rayleigh_quotient = True
# Whether keep the eigenvalues sorted during training
sort_eigvals_in_training = True

print_every_step = 10
#print_gradient_norm = True

# When true, only use (normalized) orthonality constraints in penalty term
use_reduced_2nd_penalty = False
# Whether to perform constraint step
include_constraint_step = False
# Apply constraint step after certain training steps  
constraint_first_step = 1000
# After performing constraint step, whether still include penalty term in loss function 
constraint_penalty_method = False
# Convergence criteria for constraint step
constraint_tol = 1e-2
# How often to perform constraint step
constraint_how_often = 3
# Learning rate used in constraint step
constraint_learning_rate= 1e-3
# Maximal training steps in constraint step
constraint_max_step = 100
[grid]
xmin=-3.0
xmax=3.0
nx = 500
ymin=-3.0
ymax=3.0
ny = 300
[FVD2d]
iter_n=100
error_tol=1e-2

