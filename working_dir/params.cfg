[default]
dim = 2
pot_id = 5
beta = 1.0
output_dir = ./data/
eig_file_name_prefix = eigen_vector
data_filename_prefix = states_2d
log_filename = log.txt
eig_idx_k = 1
#compute_all_k_eigs = False
compute_all_k_eigs = True
namd_data_flag = True
#namd_data_flag = False
#weights in the loss function, separated by ','
eig_weight = 1.0, 0.8, 0.6, 0.3, 0.2, 0.1
#False 
[potential]
stiff_eps = 0.50
[sample_data]
delta_t = 0.001
N_state = 1e6
[NAMD]
namd_data_path = ../../MD/alanine-dipeptide/no-abf-200ns-1/
#namd_data_path = ../../MD/alanine-dipeptide/abf-fixed-alpha0.7/
namd_data_filename_prefix = test_A
pdb_path = ../../MD/pdb-files/Alanine-Dipeptide/
pdb_prefix = A
align_data_flag = True
temperature = 300
# Dampling coefficient # (unit : ps^{-1})
damping_coeff = 1.0
# Possible value: all, nonh, angle_atoms, angle
which_data_to_use = angle_atoms
# If biased MD data are used, the files .colvar.traj 
# and .pmf are also required.
use_biased_data = False
[NeuralNetArch]
# Size of inner layers (without input/output layers)
arch_size_list = 30,30,30
ReLU_flag = False
[Training]
# If true, each processor reads part of the data
distribute_data = True
# If num_processor > 1, the training will be run in parallel. 
# In this case, pytorch with mpi is needed. 
num_processor = 1
# total training step
train_max_step = 1100
# the following lists should have the same length
# step at which a new stage starts
stage_list = 0, 500, 900, 1000
# parameters for each stage: batch-size, learning rate, and two penalty constants
batch_size_list = 10000, 50000, 5000, 10000 
learning_rate_list = 1e-3, 1e-3, 1e-3, 1e-3
alpha_1_list = 20.0, 20.0, 20.0, 30.0
alpha_2_list = 20.0, 20.0, 20.0, 30.0

# Use Rayleigh quotient or energy in loss function
use_Rayleigh_quotient = True
# Whether keep the eigenvalues sorted during training
sort_eigvals_in_training = True

print_every_step = 5
#print_gradient_norm = True

# When true, only use (normalized) orthonality constraints in penalty term
use_reduced_2nd_penalty = False
# Whether to perform constraint step
include_constraint_step = True
# After performing constraint step, whether still include penalty term in loss function 
constraint_penalty_method = False
# Convergence criteria for constraint step
constraint_tol = 1e-2
# How often to perform constraint step
constraint_how_often = 10
# Learning rate used in constraint step
constraint_learning_rate= 1e-3
# Maximal training steps in constraint step
constraint_max_step = 100
[grid]
xmin=-3.0
xmax=3.0
nx = 300
ymin=-3.0
ymax=3.0
ny = 300
[FVD2d]
iter_n=100
error_tol=1e-2

