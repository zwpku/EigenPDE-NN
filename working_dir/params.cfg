[default]
dim = 2
pot_id = 5
beta = 1.0
output_dir = ./data/
eig_file_name_prefix = eigen_vector
data_filename_prefix = states_2d
log_filename = log.txt
eig_idx_k = 1
#compute_all_k_eigs = False
compute_all_k_eigs = True
namd_data_flag = True
#namd_data_flag = False
#weights in the loss function, separated by ','
eig_weight = 1.0, 0.8, 0.6, 0.3, 0.2, 0.1
#False 
[potential]
stiff_eps = 0.50
[sample_data]
delta_t = 0.001
N_state = 1e6
[NAMD]
namd_data_path = ../../MD/alanine-dipeptide/abf-fixed-alpha0.7/
namd_data_filename_prefix = test_A
pdb_path = ../../MD/pdb-files/Alanine-Dipeptide/
pdb_prefix = A
align_data_flag = True
temperature = 300
# dampling coefficient # (unit : ps^{-1})
damping_coeff = 1.0
# possible value: all, nonh, angle_atoms, angle
which_data_to_use = angle_atoms
[NeuralNetArch]
# Size of inner layers (without input/output layers)
arch_size_list = 30,200,200
ReLU_flag = False
[Training]
# If true, each processor reads part of the data
distribute_data = True
# If num_processor > 1, the training will be run in parallel. 
# In this case, pytorch with mpi is needed. 
num_processor = 1
# total training step
train_max_step = 3100
# the following lists should have the same length
# step at which a new stage starts
stage_list = 0, 500, 900, 1000
# parameters for each stage: batch-size, learning rate, and two penalty constants
batch_size_list = 5000, 5000, 10000, 10000 
learning_rate_list = 1e-4, 1e-4, 1e-3, 1e-5
alpha_1_list = 20.0, 20.0, 20.0, 30.0
alpha_2_list = 20.0, 20.0, 20.0, 30.0

# this flag is only useful when computing multiple eigenvalues
use_Rayleigh_quotient = True
# whether keep the eigenvalues sorted during training
sort_eigvals_in_training = True
print_every_step = 10
print_gradient_norm = True
[grid]
xmin=-3.0
xmax=3.0
nx = 300
ymin=-3.0
ymax=3.0
ny = 300
[FVD2d]
iter_n=100
error_tol=1e-2

