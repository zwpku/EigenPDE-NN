[default]
dim = 2
pot_id = 5
beta = 1.0
output_dir = ./data/
eig_file_name_prefix = eigen_vector
log_filename = log.txt
eig_k = 3
#namd_data_flag = True
namd_data_flag = False
#weights in the loss function, separated by ','
eig_weight = 1.0, 0.8, 0.6, 0.3, 0.2, 0.1
#False 
load_data_how_often = 1

data_filename_prefix = states
[potential]
stiff_eps = 0.50
[sample_data]
delta_t = 0.001
N_state = 5e5
# Coefficient under which the data is sampled
SDE_beta = 1.0
[NAMD]
namd_dcddata_filename_prefix = colvar-together
#test_A
#namd_data_filename_prefix = run
psf_name = vacuum
# Possible value: none, trans, trans-rot
align_data_flag = trans-rot

temperature = 300
# Diffusion coefficient, unit is cm^2/s, default value 10^{-5}
diffusion_coeff = 1e-5
# Possible value: all, nonh, angle_atoms, angle
which_data_to_use = nonh
# If biased MD data are used, the files .colvar.traj 
# and .pmf are also required.
use_biased_data = True

weight_threshold_to_remove_states = 0

# Info for training data
# directory in which the dcd file is located. 
namd_dcddata_path = 
data_filename_prefix = states_100ns_7e-1

# Info for validataion data
namd_dcddata_path_validation = ../../MD/alanine-dipeptide/abf-fixed-100ns-0.7/
data_filename_prefix_validation = states_100ns_7e-1

[NeuralNetArch]
# Size of inner layers (without input/output layers)
arch_size_list = 20,20,20
# Activation function. 
# Possible values: ELU, LogSigmoid, ReLU, CELU, Sigmoid, Softplus, Tanh, Tanhshrink
# CELU will be used when function with given name does not exist!
activation_name = Tanh
[Training]
# total training step
train_max_step = 100
# Whether to start from a trained model 
load_init_model = False
# Filename of trained model
init_model_name = eigen_vector.pt
# the following lists should have the same length
# step at which a new stage starts
stage_list = 0, 90 
# parameters for each stage: batch-size, learning rate, and two penalty constants
batch_size_list = 5000, 20000
batch_uniform_weight = True
learning_rate_list = 5e-3, 5e-3
alpha_list = 20.0, 20.0

# Whether keep the eigenvalues sorted during training
sort_eigvals_in_training = True

print_every_step = 10
#print_gradient_norm = True

[grid]
xmin=-3.0
xmax=3.0
nx = 500
ymin=-3.0
ymax=3.0
ny = 300
[FVD2d]
iter_n=100
error_tol=1e-2
