[default]
eig_file_name_prefix = eigen_vector
log_filename = log.txt
md_data_flag = True
load_data_how_often = 1
[MD]
md_dcddata_filename_prefix = colvars
psf_name = ./MDdata/common/vacuum
# Possible value: none, trans, trans-rot
align_data_flag = none
#trans-rot
temperature = 300
# Diffusion coefficient, unit is cm^2/s, default value 10^{-5}
diffusion_coeff = 1e-5
# Possible value: all, nonh, angle_atoms, angle
which_data_to_use = nonh
# If biased MD data are used, the files .colvar.traj 
# and .pmf are also required.
use_biased_data = True

weight_threshold_to_remove_states = 0

# Info for training data
md_dcddata_path = ./MDdata/abf-fixed-100ns-0.7/
data_filename_prefix = states_100ns_7e-1-noalign

# Info for validataion data
md_dcddata_path_validation = ./MDdata/abf-fixed-100ns-0.7/
data_filename_prefix_validation = states_100ns_7e-1-test-noalign

[NeuralNetArch]
# Size of inner layers (without input/output layers)
arch_size_list = 20,20,20
# Activation function. 
# Possible values: ELU, LogSigmoid, ReLU, CELU, Sigmoid, Softplus, Tanh, Tanhshrink
# CELU will be used when function with given name does not exist!
activation_name = Tanh
[Training]
eig_k = 3
# total training step
train_max_step = 20000
# Whether to start from a trained model 
load_init_model = False
# Filename of trained model
init_model_name = eigen_vector.pt
#weights in the loss function, separated by ','
eig_weight = 1.0, 0.8, 0.6, 0.3, 0.2, 0.1
# the following lists should have the same length
# step at which a new stage starts
stage_list = 0, 4000, 8000, 12000, 16000 
# parameters for each stage: batch-size, learning rate, and two penalty constants
batch_size_list = 10000, 10000, 10000, 10000, 10000
batch_uniform_weight = True
learning_rate_list = 1e-3, 1e-3, 1e-3, 1e-3, 1e-3
alpha_list = 20.0, 20.0, 20.0, 20.0, 20.0

# Whether keep the eigenvalues sorted during training
sort_eigvals_in_training = True

print_every_step = 10
[grid]
xmin=-3.0
xmax=3.0
nx = 500
ymin=-3.0
ymax=3.0
ny = 300
